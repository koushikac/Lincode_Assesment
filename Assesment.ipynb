{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f9aafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import swat\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from PIL import Image\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "\tdef __init__(self, root, labels_path, transform, all_labels):\n",
    "\t\tself.all_labels = all_labels\n",
    "\t\tself.data = self.make_data(labels_path)\n",
    "\t\tself.root = root\n",
    "\t\tself.data_dict = self.create_dict()\n",
    "\t\tself.transform = transform\n",
    "\t\n",
    "\tdef __len__(self):\n",
    "\t\treturn len(self.data)\n",
    "\t\n",
    "\tdef get_labels(self):\n",
    "\t\treturn self.labels\n",
    "\t\n",
    "\tdef create_dict(self):\n",
    "\t\tdata_dict = defaultdict(list)\n",
    "\t\tfor index, datum in self.data.iterrows():\n",
    "\t\t\tdata_dict[datum[' LABEL']].append(os.path.join(self.root, datum['IMAGE_FILENAME']))\n",
    "\t\treturn data_dict\n",
    "\t\n",
    "\tdef make_data(self, labels_path):\n",
    "\t\tdata = pd.read_csv(labels_path)\n",
    "\t\t\n",
    "\t\tnew_rows = []\n",
    "\t\tfor index, datum in data.iterrows():\n",
    "\t\t\tnew_rows.append({'IMAGE_FILENAME': datum['IMAGE_FILENAME'], ' LABEL': self.all_labels.index(datum[' LABEL'])})\n",
    "\t\treturn pd.DataFrame(new_rows)\n",
    "\t\n",
    "\tdef __getitem__(self, index):\n",
    "\t\timage_path = self.data['IMAGE_FILENAME'][index]\n",
    "\t\tlabel = self.data[' LABEL'][index]\n",
    "\t\timage = Image.open(os.path.join(self.root, image_path))  # only load the grayscale image\n",
    "\t\timage = self.transform(image)\n",
    "\t\treturn image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f50545",
   "metadata": {},
   "source": [
    "#Read data and visualise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0d5444",
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "import cv2\n",
    "import torch \n",
    "import os\n",
    "import swat\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b97ae12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from PIL import Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e43e01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = 'data/images'\n",
    "labels_path = 'data/gicsd_labels.csv'\n",
    "data = pd.read_csv(labels_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1e2e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_image_files = [ datum['IMAGE_FILENAME'] for index, datum in data.iterrows()]\n",
    "all_image_labels = [datum[' LABEL'] for index, datum in data.iterrows()]\n",
    "\n",
    "\n",
    "all_images = [cv2.imread(os.path.join(root,file)) for file in all_image_files]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af351ac",
   "metadata": {},
   "source": [
    "Check if all the images are the same size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199e4a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "image_shapes = set([image.shape for image in all_images])\n",
    "print(image_shapes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666dda40",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_dict = defaultdict(list)\n",
    "for index, datum in data.iterrows():\n",
    "    data_dict[datum[' LABEL']].append(os.path.join(root, datum['IMAGE_FILENAME'])) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75cfa3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, images in data_dict.items():\n",
    "    print(key, len(images))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed44a34",
   "metadata": {},
   "source": [
    "Visualise a few examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5fe7c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, images in data_dict.items():\n",
    "    frames = [cv2.imread(im) for im in images[:5]]\n",
    "    print(key)\n",
    "    plt.imshow(np.asarray(np.hstack(frames)))\n",
    "    plt.show("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723d32ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "colours = ['b','g','r'] \n",
    "\n",
    "for key, images in data_dict.items():\n",
    "    print(key)\n",
    "    frames = [cv2.imread(im) for im in images[:5]]\n",
    "    for frame in frames: \n",
    "        chans = cv2.split(frame)\n",
    "        for (chan, color) in zip(chans, colours):   \n",
    "            hist = cv2.calcHist([chan],[0], None, [255], [0, 256])\n",
    "            plt.plot(hist, color = color)\n",
    "            plt.xlim([0, 256])\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc099a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, images in data_dict.items():\n",
    "    frames = [cv2.imread(im) for im in images[:5]]\n",
    "    blues = [frame[:,:,0] for frame in frames]\n",
    "    greens = [frame[:,:,1] for frame in frames]\n",
    "    reds = [frame[:,:,2] for frame in frames]\n",
    "\n",
    "\n",
    "    print(key)\n",
    "    plt.imshow(np.asarray(np.hstack(blues)))\n",
    "    plt.show()\n",
    "    plt.imshow(np.asarray(np.hstack(greens)))\n",
    "    plt.show()\n",
    "    plt.imshow(np.asarray(np.hstack(reds)))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d4ccf0",
   "metadata": {},
   "source": [
    "Save the blue channel of the data as greyscale images in the folder data/images_grayscale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4373b427",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_root = 'data/images_grayscale'\n",
    "os.makedirs(new_root, exist_ok=True)\n",
    "all_images = [im[:,:,0] for im in all_images]\n",
    "for im, filename in zip(all_images, all_image_files):\n",
    "    cv2.imwrite(os.path.join(new_root, filename), im)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7752261",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grayscale_channel(img_path):\n",
    "    img = cv2.imread(img_path) \n",
    "    return img[:,:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea4075f1",
   "metadata": {},
   "source": [
    "Split the data into training, testing and validation dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2acc41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataframe by label and shuffle it\n",
    "data_fv= data[data[' LABEL']==' FULL_VISIBILITY '].reset_index(drop=True)\n",
    "data_pv= data[data[\" LABEL\"]==' PARTIAL_VISIBILITY '].reset_index(drop=True)\n",
    "data_nc= data[data[\" LABEL\"]==' NO_VISIBILITY '].reset_index(drop=True)\n",
    "\n",
    "test = data_fv.loc[:int(len(data_fv)*.1),:]\n",
    "test = test.append(data_pv.loc[:int(len(data_pv)*.1),:], ignore_index=True)\n",
    "test = test.append(data_nc.loc[:9,:], ignore_index=True)\n",
    "test.to_csv('data/gicsd_labels_test.csv')\n",
    "\n",
    "data_fv = data_fv.loc[int(len(data_fv)*.1)+1:,:]\n",
    "data_pv = data_pv.loc[int(len(data_pv)*.1)+1:,:]\n",
    "data_nc = data_nc.loc[10:,:]\n",
    "\n",
    "print(test.shape)\n",
    "print(data_fv.shape)\n",
    "print(data_pv.shape)\n",
    "print(data_nc.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f54fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e858e0",
   "metadata": {},
   "source": [
    "Augment data in preparation for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a71ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = 'data/images_grayscale'\n",
    "from utils import rotate_data, flip_data, mirror_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97dd88a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_nc = rotate_data(data_nc, root)\n",
    "data_nc = flip_data(data_nc, root)\n",
    "data_nc = mirror_data(data_nc, root)\n",
    "data_nc.to_csv('data/data_nc2.csv')\n",
    "\n",
    "print(data_nc.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ac8cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_pv = rotate_data(data_pv, root)\n",
    "data_pv.to_csv('data/data_pv2.csv')\n",
    "\n",
    "\n",
    "print(data_pv.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2b2a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = data_fv.append(data_pv, ignore_index=True)\n",
    "train = train.append(data_nc, ignore_index=True).sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "validation = train.loc[:int(len(train)*.1),:]\n",
    "train = train.loc[int(len(train)*.1)+1:,:]\n",
    "\n",
    "\n",
    "train.to_csv('data/gicsd_labels_train.csv')\n",
    "validation.to_csv('data/gicsd_labels_val.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0b5d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from train import Model\n",
    "from torchvision import transforms\n",
    "from dataset import Dataset\n",
    "model = Model()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2071ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "checkpoint = torch.load('resnet34.pth')\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "all_labels = [' FULL_VISIBILITY ', ' PARTIAL_VISIBILITY ', ' NO_VISIBILITY ']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6345e576",
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix(model, dataloader, all_labels):\n",
    "    nb_classes = len(all_labels)\n",
    "\n",
    "    confusion_matrix = torch.zeros(nb_classes, nb_classes)\n",
    "    with torch.no_grad():\n",
    "        for i, (inputs, classes) in enumerate(dataloader):\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            for t, p in zip(classes.view(-1), preds.view(-1)):\n",
    "                confusion_matrix[t.long(), p.long()] += 1\n",
    "    print(\"Confusion matrix:\")\n",
    "    print(confusion_matrix)\n",
    "    print(\"Per class accuracy:\")\n",
    "    print(confusion_matrix.diag()/confusion_matrix.sum(1))\n",
    "    return confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d0e48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.Resize(256), transforms.ToTensor(), transforms.Normalize(mean=[0.485], std=[0.229])])\n",
    "\n",
    "test_data = Dataset('/Users/vira/Documents/ml/revolut/data/images_grayscale', '/Users/vira/Documents/ml/revolut/data/gicsd_labels_test.csv', transform, all_labels = [' FULL_VISIBILITY ', ' PARTIAL_VISIBILITY ', ' NO_VISIBILITY '])\n",
    "test_dataloader = torch.utils.data.DataLoader(test_data, batch_size=32, shuffle=True)\n",
    "\n",
    "confusion = confusion_matrix(model, test_dataloader, all_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b94d1463",
   "metadata": {},
   "source": [
    "Additional data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b0e248",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_np = np.asarray(all_images)\n",
    "data_np = data_np / 255.0\n",
    "unique_labels, _, numeric_image_labels = np.unique(all_image_labels, return_index=True, return_inverse=True)\n",
    "\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6f6a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap.umap_ as umap\n",
    "\n",
    "\n",
    "reducer = umap.UMAP()\n",
    "embedding = reducer.fit_transform(data_np.reshape((data_np.shape[0], data_np.shape[1]*data_np.shape[2])))\n",
    "plt.scatter(embedding[:,0], embedding[:,1], c =numeric_image_labels )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c5b15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "tsne = TSNE(n_components=2, verbose=1, perplexity=40, n_iter=300)\n",
    "tsne_results = tsne.fit_transform(data_np.reshape(\n",
    "    (data_np.shape[0], data_np.shape[1]*data_np.shape[2])))\n",
    "plt.scatter(tsne_results[:,0], tsne_results[:,1], c =numeric_image_labels )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1207aa2b",
   "metadata": {},
   "source": [
    "Morphological edge detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f7c5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "for img in all_images[:5]:\n",
    "    edges = cv2.Canny(img,100,200)\n",
    "    plt.imshow(edges,cmap = 'gray')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8de566b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import argparse\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "from train import Model\n",
    "\n",
    "parser = argparse.ArgumentParser(description='Process some images.')\n",
    "parser.add_argument('-image_path', type=str, required=True, help='path to the image for inference')\n",
    "parser.add_argument('-model_weights', type=str, default='best_checkpointt.pth', help='path to the image for inference')\n",
    "\n",
    "\n",
    "def predict(args):\n",
    "\t# get only the grayscale image that is stored in the blue channel\n",
    "\n",
    "\timg = Image.open(args.image_path)\n",
    "\tif img.mode == 'RGB':\n",
    "\t\tprint(\"Only taking blue channel from RGB image\")\n",
    "\t\timg = img.split()[2]\n",
    "\telif img.mode == 'L':\n",
    "\t\tprint(\"Provided image is already grayscale\")\n",
    "\n",
    "\t\n",
    "\ttransform = transforms.Compose([transforms.Resize(256), transforms.ToTensor(),\n",
    "\t                                transforms.Normalize(mean=[0.485], std=[0.229])])\n",
    "\t\n",
    "\t# img = img.split()[2]\n",
    "\timg_t = transform(img)\n",
    "\tbatch_t = torch.unsqueeze(img_t, 0)\n",
    "\t\n",
    "\tmodel = Model()\n",
    "\t\n",
    "\tcheckpoint = torch.load(args.model_weights)\n",
    "\tmodel.load_state_dict(checkpoint['state_dict'])\n",
    "\t\n",
    "\tmodel.eval()\n",
    "\tout = model(batch_t)\n",
    "\t_, preds = torch.max(out, 1)\n",
    "\t\n",
    "\tprint('Result: ', args.all_labels[preds.item()])\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\tparser = argparse.ArgumentParser(description='Process some integers.')\n",
    "\tparser.add_argument('-image_path', type=str, required=True, help='path to the image for inference')\n",
    "\tparser.add_argument('-model_weights', type=str, default='resnet34.pth', help='path to the image for inference')\n",
    "\tparser.add_argument('-all_labels', type=list, default=[' FULL_VISIBILITY ', ' PARTIAL_VISIBILITY ', ' NO_VISIBILITY '], help='The list of labels')\n",
    "\targs = parser.parse_args()\n",
    "\t\n",
    "\tpredict(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50e7587",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import models\n",
    "from dataset import Dataset\n",
    "from torchvision import transforms\n",
    "from torch.autograd import Variable\n",
    "import argparse\n",
    "\n",
    "\n",
    "class Model(torch.nn.Module):\n",
    "\tdef __init__(self):\n",
    "\t\tsuper(Model, self).__init__()\n",
    "\t\t\n",
    "\t\tself.resnet = models.resnet34(pretrained=False)\n",
    "\t\tself.resnet.conv1 = torch.nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3,\n",
    "\t\t                                    bias=False)  # change to single channel input\n",
    "\t\t\n",
    "\t\tself.resnet = load_weights_single_channel(self.resnet,\n",
    "\t\t                                          \"https://download.pytorch.org/models/resnet34-333f7ec4.pth\")\n",
    "\t\t\n",
    "\t\tfor param in self.resnet.parameters():\n",
    "\t\t\tparam.requires_grad = True\n",
    "\t\t\n",
    "\t\tself.resnet.fc = torch.nn.Linear(self.resnet.fc.in_features, 3)  # change to 3 class classification\n",
    "\t\t\n",
    "\t\tself.softmax = torch.nn.Softmax()\n",
    "\t\n",
    "\tdef forward(self, x):\n",
    "\t\tx = self.resnet(x)\n",
    "\t\tx = self.softmax(x)\n",
    "\t\treturn x\n",
    "\n",
    "\n",
    "class AverageMeter(object):\n",
    "\t\"\"\"Computes and stores the average and current value\"\"\"\n",
    "\t\n",
    "\tdef __init__(self):\n",
    "\t\tself.reset()\n",
    "\t\n",
    "\tdef reset(self):\n",
    "\t\tself.val = 0\n",
    "\t\tself.avg = 0\n",
    "\t\tself.sum = 0\n",
    "\t\tself.count = 0\n",
    "\t\n",
    "\tdef update(self, val, n=1):\n",
    "\t\tself.val = val\n",
    "\t\tself.sum += val * n\n",
    "\t\tself.count += n\n",
    "\t\tself.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "def set_parameter_requires_grad(model, feature_extracting):\n",
    "\tif feature_extracting:\n",
    "\t\tfor param in model.parameters():\n",
    "\t\t\tparam.requires_grad = False\n",
    "\n",
    "\n",
    "def load_weights_single_channel(model, url):\n",
    "\tstate_dict = torch.utils.model_zoo.load_url(url)\n",
    "\tconv1_weight = state_dict['conv1.weight']\n",
    "\tstate_dict['conv1.weight'] = conv1_weight.sum(dim=1, keepdim=True)\n",
    "\tmodel.load_state_dict(state_dict)\n",
    "\treturn model\n",
    "\n",
    "\n",
    "def main(args):\n",
    "\ttransform = transforms.Compose([transforms.Resize(256), transforms.ToTensor(), transforms.Normalize(mean=[0.485], std=[0.229])])\n",
    "\t\n",
    "\ttrain_data = Dataset(args.root, args.annotations_train, transform, args.all_labels)\n",
    "\tval_data = Dataset(args.root, args.annotations_val, transform, args.all_labels)\n",
    "\tdataloader = torch.utils.data.DataLoader(train_data, batch_size=args.batch_size, shuffle=True)\n",
    "\tval_dataloader = torch.utils.data.DataLoader(val_data, batch_size=args.batch_size, shuffle=True)\n",
    "\t\n",
    "\tmodel = Model()\n",
    "\tweight = torch.tensor([1.0, 2.0, 2.0])\n",
    "\tcriterion = torch.nn.CrossEntropyLoss()\n",
    "\toptimiser = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\tlr_scheduler = torch.optim.lr_scheduler.StepLR(optimiser, step_size=7, gamma=0.1)\n",
    "\t\n",
    "\tbest_acc = 0\n",
    "\tfor epoch in range(args.epochs):\n",
    "\t\ttrain_loss, train_accuracy = train(model, criterion, optimiser, lr_scheduler, dataloader, epoch, args.epochs)\n",
    "\t\t\n",
    "\t\tvalidation_loss, acc = validation(model, criterion, optimiser, lr_scheduler, val_dataloader, epoch, args.epochs)\n",
    "\t\tif acc > best_acc:\n",
    "\t\t\tbest_acc = acc\n",
    "\t\t\tstate = {\n",
    "\t\t\t\t'epoch': epoch,\n",
    "\t\t\t\t'state_dict': model.state_dict(),\n",
    "\t\t\t\t'optimizer': optimiser.state_dict(),\n",
    "\t\t\t\t'best_acc': best_acc\n",
    "\t\t\t}\n",
    "\t\t\ttorch.save(state, args.checkpoint)\n",
    "\n",
    "\n",
    "def train(model, criterion, optimiser, scheduler, dataloader, epoch, epochs):\n",
    "\tlosses = AverageMeter()\n",
    "\taccuracies = AverageMeter()\n",
    "\t\n",
    "\tmodel.train()  # Set model to training mode\n",
    "\tfor batch, (inputs, labels) in enumerate(dataloader):\n",
    "\t\t\n",
    "\t\tinputs = Variable(inputs)\n",
    "\t\tlabels = Variable(labels)\n",
    "\t\t\n",
    "\t\t# print(labels)\n",
    "\t\toutputs = model(inputs)\n",
    "\t\t# _, preds = torch.max(outputs, 1)\n",
    "\t\tloss = criterion(outputs, labels)\n",
    "\t\t\n",
    "\t\t# calculate accuracies\n",
    "\t\tacc = calculate_accuracy(outputs, labels)\n",
    "\t\t\n",
    "\t\t# statistics\n",
    "\t\tlosses.update(loss.item(), inputs.size(0))\n",
    "\t\taccuracies.update(acc, inputs.size(0))\n",
    "\t\t\n",
    "\t\toptimiser.zero_grad()\n",
    "\t\tloss.backward()\n",
    "\t\toptimiser.step()\n",
    "\t\tscheduler.step()\n",
    "\t\t\n",
    "\t\tif batch % 10 == 0:\n",
    "\t\t\tprint('Epoch {}/{}:[{}]/[{}] Loss: {:.4f} Acc: {:.4f}'.format(epoch, epochs, batch, len(dataloader),\n",
    "\t\t\t                                                              losses.avg, accuracies.avg))\n",
    "\t\n",
    "\treturn losses.avg, accuracies.avg\n",
    "\n",
    "\n",
    "def validation(model, criterion, optimiser, scheduler, dataloader, epoch, epochs):\n",
    "\tlosses = AverageMeter()\n",
    "\taccuracies = AverageMeter()\n",
    "\tmodel.eval()  # Set model to validation mode\n",
    "\tfor batch, (inputs, labels) in enumerate(dataloader):\n",
    "\t\toutputs = model(inputs)\n",
    "\t\t# _, preds = torch.max(outputs, 1)\n",
    "\t\tloss = criterion(outputs, labels)\n",
    "\t\t\n",
    "\t\t# calculate accuracies\n",
    "\t\tacc = calculate_accuracy(outputs, labels)\n",
    "\t\t# precision = calculate_precision(outputs, labels)  #\n",
    "\t\t# recall = calculate_recall(outputs, labels)\n",
    "\t\t\n",
    "\t\tlosses.update(loss.item(), inputs.size(0))\n",
    "\t\taccuracies.update(acc, inputs.size(0))\n",
    "\t\tif batch % 10 == 0:\n",
    "\t\t\tprint('Val epoch {}/{}:[{}]/[{}] Loss: {:.4f} Acc: {:.4f}'.format(epoch, epochs, batch, len(dataloader), losses.avg, accuracies.avg))\n",
    "\t\n",
    "\treturn losses.avg, accuracies.avg\n",
    "\n",
    "\n",
    "def calculate_accuracy(outputs, targets):\n",
    "\tbatch_size = targets.size(0)\n",
    "\t\n",
    "\t_, pred = outputs.topk(1, 1, True)\n",
    "\tpred = pred.t()\n",
    "\tcorrect = pred.eq(targets.view(1, -1))\n",
    "\tn_correct_elems = correct.float().sum().item()\n",
    "\t\n",
    "\treturn n_correct_elems / batch_size\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\tall_labels = [' FULL_VISIBILITY ', ' PARTIAL_VISIBILITY ', ' NO_VISIBILITY ']\n",
    "\t\n",
    "\tparser = argparse.ArgumentParser(description='Process some integers.')\n",
    "\tparser.add_argument('-epochs', type=int, default=20, help='number of epoch for training')\n",
    "\tparser.add_argument('-batch_size', type=int, default=16, help='number of epoch for training')\n",
    "\tparser.add_argument('-checkpoint', type=str, default='resnet34.pth', help='path where to save checkpoint during training')\n",
    "\tparser.add_argument('-root', type=str, default='data/images_grayscale', help='path to the folder with grayscale images')\n",
    "\tparser.add_argument('-annotations_train', type=str, default='data/gicsd_labels_train.csv', help='path to the folder with grayscale images')\n",
    "\tparser.add_argument('-annotations_val', type=str, default='data/gicsd_labels_val.csv', help='path to the folder with grayscale images')\n",
    "\tparser.add_argument('-all_labels', type=list, default=[' FULL_VISIBILITY ', ' PARTIAL_VISIBILITY ', ' NO_VISIBILITY '], help='The list of labels')\n",
    "\targs = parser.parse_args()\n",
    "\tmain(args)\n",
    "\tprint('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5341c7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def mirror_data(dataframe, root):\n",
    "\t\"\"\"\n",
    "\tCreates a mirror reflection for  every image in the dataframe and save new images in the same\n",
    "\tfolder and adds the new image into the dataframe\n",
    "\t:param dataframe: dataframe with filenames and labels\n",
    "\t:param root: path to the folder with all the images\n",
    "\t:return:\n",
    "\t\"\"\"\n",
    "\tnew_entries = []\n",
    "\tfor index, datum in dataframe.iterrows():\n",
    "\t\timg = cv2.imread(os.path.join(root, datum['IMAGE_FILENAME']), cv2.IMREAD_UNCHANGED)\n",
    "\t\tnew_image_name = datum['IMAGE_FILENAME'].split(\".\")[0] + 'm.png'\n",
    "\t\tflipped_image = np.fliplr(img)\n",
    "\t\tcv2.imwrite(os.path.join(root, new_image_name), flipped_image)\n",
    "\t\tnew_entries.append({'IMAGE_FILENAME': new_image_name, ' LABEL': datum[' LABEL']})\n",
    "\tdf = pd.DataFrame(new_entries)\n",
    "\treturn dataframe.append(df, ignore_index=True)\n",
    "\n",
    "\n",
    "def flip_data(dataframe, root):\n",
    "\t\"\"\"\n",
    "\tTurns upside down every image in the dataframe and save new images in the same\n",
    "\tfolder and adds the new image into the dataframe\n",
    "\t:param dataframe: dataframe with filenames and labels\n",
    "\t:param root: path to the folder with all the images\n",
    "\t:return: new dataframe dataframe\n",
    "\t\"\"\"\n",
    "\tnew_entries = []\n",
    "\tfor index, datum in dataframe.iterrows():\n",
    "\t\timg = cv2.imread(os.path.join(root, datum['IMAGE_FILENAME']), cv2.IMREAD_UNCHANGED)\n",
    "\t\tnew_image_name = datum['IMAGE_FILENAME'].split(\".\")[0] + 'f.png'\n",
    "\t\t\n",
    "\t\tflipped_image = np.flipud(img)\n",
    "\t\t\n",
    "\t\tcv2.imwrite(os.path.join(root, new_image_name), flipped_image)\n",
    "\t\tnew_entries.append({'IMAGE_FILENAME': new_image_name, ' LABEL': datum[' LABEL']})\n",
    "\tdf = pd.DataFrame(new_entries)\n",
    "\treturn dataframe.append(df, ignore_index=True)\n",
    "\n",
    "\n",
    "def rotate_data(dataframe, root):\n",
    "\t\"\"\"\n",
    "\tRotate every image in the dataframe and save new images in the same\n",
    "\tfolder and adds the new image into the dataframe\n",
    "\t:param dataframe: dataframe with filenames and labels\n",
    "\t:param root: path to the folder with all the images\n",
    "\t:return:\n",
    "\t\"\"\"\n",
    "\tnew_entries = []\n",
    "\tfor index, datum in dataframe.iterrows():\n",
    "\t\timg = cv2.imread(os.path.join(root, datum['IMAGE_FILENAME']), cv2.IMREAD_UNCHANGED)\n",
    "\t\tnew_image_name = datum['IMAGE_FILENAME'].split(\".\")[0] + 'r.png'\n",
    "\t\tangle = random.uniform(-45, 45)\n",
    "\t\timage_rotated_cropped = rotate_resize(img, angle)\n",
    "\t\tcv2.imwrite(os.path.join(root, new_image_name), image_rotated_cropped)\n",
    "\t\tnew_entries.append({'IMAGE_FILENAME': new_image_name, ' LABEL': datum[' LABEL']})\n",
    "\tdf = pd.DataFrame(new_entries)\n",
    "\treturn dataframe.append(df, ignore_index=True)\n",
    "\n",
    "\n",
    "def rotate_resize(img, angle):\n",
    "\t\"\"\"\n",
    "\tRotates the images, crops the black border out and resizes to the shape of the original image\n",
    "\t\"\"\"\n",
    "\timage_height, image_width = img.shape\n",
    "\timage_rotated = rotate_image(img, angle)\n",
    "\timage_rotated_cropped = crop_around_center(image_rotated,\n",
    "\t                                           *largest_rotated_rect(image_width, image_height, math.radians(angle)))\n",
    "\treturn cv2.resize(image_rotated_cropped, img.shape, interpolation=cv2.INTER_AREA)\n",
    "\n",
    "\n",
    "def rotate_image(img, angle):\n",
    "\t\"\"\"\n",
    "\tRotate image\n",
    "\t\"\"\"\n",
    "\t\n",
    "\trows, cols = img.shape\n",
    "\trotation_matrix = cv2.getRotationMatrix2D((cols / 2, rows / 2), angle, 1)\n",
    "\treturn cv2.warpAffine(img, rotation_matrix, (rows, cols))\n",
    "\n",
    "\n",
    "def largest_rotated_rect(w, h, angle):\n",
    "\t\"\"\"\n",
    "\tGiven a rectangle of size wxh that has been rotated by 'angle' (in\n",
    "\tradians), computes the width and height of the largest possible\n",
    "\taxis-aligned rectangle within the rotated rectangle.\n",
    "\tOriginal JS code by 'Andri' and Magnus Hoff from Stack Overflow\n",
    "\tConverted to Python by Aaron Snoswell\n",
    "\t\"\"\"\n",
    "\t\n",
    "\tquadrant = int(math.floor(angle / (math.pi / 2))) & 3\n",
    "\tsign_alpha = angle if ((quadrant & 1) == 0) else math.pi - angle\n",
    "\talpha = (sign_alpha % math.pi + math.pi) % math.pi\n",
    "\t\n",
    "\tbb_w = w * math.cos(alpha) + h * math.sin(alpha)\n",
    "\tbb_h = w * math.sin(alpha) + h * math.cos(alpha)\n",
    "\t\n",
    "\tgamma = math.atan2(bb_w, bb_w) if (w < h) else math.atan2(bb_w, bb_w)\n",
    "\t\n",
    "\tdelta = math.pi - alpha - gamma\n",
    "\t\n",
    "\tlength = h if (w < h) else w\n",
    "\t\n",
    "\td = length * math.cos(alpha)\n",
    "\ta = d * math.sin(alpha) / math.sin(delta)\n",
    "\t\n",
    "\ty = a * math.cos(gamma)\n",
    "\tx = y * math.tan(gamma)\n",
    "\t\n",
    "\treturn (\n",
    "\t\tbb_w - 2 * x,\n",
    "\t\tbb_h - 2 * y\n",
    "\t)\n",
    "\n",
    "\n",
    "def crop_around_center(image, width, height):\n",
    "\t\"\"\"\n",
    "\tGiven a NumPy / OpenCV 2 image, crops it to the given width and height,\n",
    "\taround it's centre point\n",
    "\t\"\"\"\n",
    "\t\n",
    "\timage_size = (image.shape[1], image.shape[0])\n",
    "\timage_center = (int(image_size[0] * 0.5), int(image_size[1] * 0.5))\n",
    "\t\n",
    "\tif (width > image_size[0]):\n",
    "\t\twidth = image_size[0]\n",
    "\t\n",
    "\tif (height > image_size[1]):\n",
    "\t\theight = image_size[1]\n",
    "\t\n",
    "\tx1 = int(image_center[0] - width * 0.5)\n",
    "\tx2 = int(image_center[0] + width * 0.5)\n",
    "\ty1 = int(image_center[1] - height * 0.5)\n",
    "\ty2 = int(image_center[1] + height * 0.5)\n",
    "\t\n",
    "\treturn image[y1:y2, x1:x2]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
